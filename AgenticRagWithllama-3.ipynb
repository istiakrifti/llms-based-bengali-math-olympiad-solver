{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"\n  font-size: 3em;\n  color: #333;\n  text-align: center;\n  background-color: #f2f2f2;\n  padding: 20px;\n  border-radius: 10px;\n  margin: 40px;\n  animation: fadeIn 1s ease-in-out;\n\">Agentic RAG with LangGrapgh ðŸŽ‰</h1>","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"markdown","source":"<div style=\"background-color: #f9f9f9; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\">\n  <p style=\"font-size: 16px; line-height: 1.6; color: #555;\">\n    In this notebook, I built a RAG (Retrieval Augmented Generation) system to answer medical queries. The agent's primary duty is to fetch chunks from a vector store to answer a user's query. However, the retrieved chunks are not just given as-is to the language model (LLM). They are first scored, and only the chunks that are relevant to the query are passed to the LLM.\n  </p>\n  <p style=\"font-size: 16px; line-height: 1.6; color: #555;\">\n    In cases where all the retrieved chunks are not relevant to the query, the agent performs a web search. The agent also performs a web search for queries that do not have any relevant documents in the vector store.\n  </p>\n  <p style=\"font-size: 16px; line-height: 1.6; color: #555;\">\n    Before the LLM's response is presented to the user, the system performs a hallucination and answer relevance check. Lastly, if the query is not related to health, the system falls back to a different chain to respond based on its knowledge.\n  </p>\n  <h3 style=\"color: #333; margin-top: 30px;\">Stack Used:</h3>\n  <ul style=\"font-size: 16px; line-height: 1.6; color: #555; list-style-type: none; padding-left: 0;\">\n    <li>- Chromadb</li>\n    <li>- Tavily AI</li>\n    <li>- HuggingFaceHubEmbeddings (sentence-transformers/all-mpnet-base-v2)</li>\n    <li>- Llama 3 70B from Groq</li>\n    <li>- Langchain and LangGraph</li>\n    <li>- Gradio</li>\n  </ul>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"\n  font-size: 2em;\n  color: #333;\n  background-color: #f2f2f2;\n  padding: 15px;\n  border-radius: 10px;\n  margin: 20px;\n  animation: fadeIn 1s ease-in-out;\n  width: 420px;\n\">1. Importing relevant libraries</h1>","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain\\\n    langchain-community\\\n    langchain-groq\\\n    langchain-core\\\n    gpt4all\\\n    langgraph\\\n    chromadb\\\n    sentence-transformers\\\n    tavily-python\\\n    gradio\\\n    langchain-huggingface","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:35:14.521959Z","iopub.execute_input":"2024-06-07T21:35:14.522395Z","iopub.status.idle":"2024-06-07T21:36:35.754302Z","shell.execute_reply.started":"2024-06-07T21:35:14.522344Z","shell.execute_reply":"2024-06-07T21:36:35.753113Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom dotenv import load_dotenv\nfrom langchain_community.document_loaders.web_base import WebBaseLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport nest_asyncio\nfrom langchain_community.vectorstores.chroma import Chroma\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings\nfrom langchain_groq.chat_models import ChatGroq\nfrom langchain_community.tools import DuckDuckGoSearchResults\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom typing import Literal\nfrom langchain.chains.combine_documents import stuff\nfrom operator import itemgetter\nfrom langchain_core.output_parsers import StrOutputParser\nfrom IPython.core.display import Markdown\nimport json\nimport re\nfrom langchain_core.runnables import (\n    RunnableParallel,\n    RunnableBranch,\n    RunnablePassthrough,\n)\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom operator import itemgetter\nimport asyncio\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:36:35.757063Z","iopub.execute_input":"2024-06-07T21:36:35.757442Z","iopub.status.idle":"2024-06-07T21:36:36.977821Z","shell.execute_reply.started":"2024-06-07T21:36:35.757407Z","shell.execute_reply":"2024-06-07T21:36:36.97672Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1 style=\"\n  font-size: 2em;\n  color: #333;\n  background-color: #f2f2f2;\n  padding: 15px;\n  border-radius: 10px;\n  margin: 20px;\n  animation: fadeIn 1s ease-in-out;\n  width: 400px;\n\">2. Set Environment Variables</h1>","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport os\nuser_secrets = UserSecretsClient()\nGROQ_API_KEY = user_secrets.get_secret(\"GROQ_API_KEY\")\nLANGCHAIN_API_KEY = user_secrets.get_secret(\"LANGCHAIN_API_KEY\")\nLANGCHAIN_PROJECT = user_secrets.get_secret(\"LANGCHAIN_PROJECT\")\nTAVILY_API_KEY = user_secrets.get_secret(\"TAVILY_API_KEY\")\n\nos.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\nos.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\nos.environ[\"LANGCHAIN_API_KEY\"]=LANGCHAIN_API_KEY\nos.environ[\"LANGCHAIN_PROJECT\"]=\"Agentic RAG\"\nos.environ[\"TAVILY_API_KEY\"]=TAVILY_API_KEY","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:36:36.97909Z","iopub.execute_input":"2024-06-07T21:36:36.979543Z","iopub.status.idle":"2024-06-07T21:36:37.705986Z","shell.execute_reply.started":"2024-06-07T21:36:36.979511Z","shell.execute_reply":"2024-06-07T21:36:37.704766Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Utility Function","metadata":{}},{"cell_type":"code","source":"def parse_search_research(results: str):\n    pattern = r\"\\[content: (.*?), title: (.*?), url: (.*?)\\]\"\n    result = re.findall(pattern, results)\n\n    data_list = []\n    for snippet, title, link in result:\n        data_list.append({\"content\": snippet, \"title\": title, \"url\": link})\n    return data_list","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:36:37.707352Z","iopub.execute_input":"2024-06-07T21:36:37.707703Z","iopub.status.idle":"2024-06-07T21:36:37.71425Z","shell.execute_reply.started":"2024-06-07T21:36:37.707673Z","shell.execute_reply":"2024-06-07T21:36:37.713053Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1 style=\"\n  font-size: 2em;\n  color: #333;\n  background-color: #f2f2f2;\n  padding: 15px;\n  border-radius: 10px;\n  margin: 20px;\n  animation: fadeIn 1s ease-in-out;\n  width: 600px;\n\">3. Preparing and Storing Data in Vector Store.</h1>","metadata":{}},{"cell_type":"code","source":"urls = [\n    \"https://www.webmd.com/a-to-z-guides/malaria\",\n    \"https://www.webmd.com/diabetes/type-1-diabetes\",\n    \"https://www.webmd.com/diabetes/type-2-diabetes\",\n    \"https://www.webmd.com/migraines-headaches/migraines-headaches-migraines\",\n]\n\nloader = WebBaseLoader(urls, bs_get_text_kwargs={\"strip\": True})\ndocs = loader.load()","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:36:37.718331Z","iopub.execute_input":"2024-06-07T21:36:37.7188Z","iopub.status.idle":"2024-06-07T21:36:39.545315Z","shell.execute_reply.started":"2024-06-07T21:36:37.718758Z","shell.execute_reply":"2024-06-07T21:36:39.544129Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)\nchunks = text_splitter.split_documents(docs)\n\nembedding_function = HuggingFaceEmbeddings(show_progress=True, multi_process=True)\n\nvector_store = Chroma.from_documents(documents=chunks, embedding=embedding_function)\n\nretriever = vector_store.as_retriever(search_kwargs={\"k\": 3})","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:36:39.547254Z","iopub.execute_input":"2024-06-07T21:36:39.54771Z","iopub.status.idle":"2024-06-07T21:38:35.017363Z","shell.execute_reply.started":"2024-06-07T21:36:39.54767Z","shell.execute_reply":"2024-06-07T21:38:35.016307Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# retriever.get_relevant_documents(\"Symptoms of migraine\")","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:38:35.018665Z","iopub.execute_input":"2024-06-07T21:38:35.019515Z","iopub.status.idle":"2024-06-07T21:38:35.024835Z","shell.execute_reply.started":"2024-06-07T21:38:35.019475Z","shell.execute_reply":"2024-06-07T21:38:35.023753Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n  font-size: 1em;\n  color: #333;\n  background-color: #f2f2f2;\n  padding: 10px;\n  border-radius: 10px;\n  margin: 20px;\n  animation: fadeIn 1s ease-in-out;\n  width: 500px;\n\">\n    <h1>4. Question Router Chain.</h1>\n    <p style=\"font-size: 1.2em;\"\n        >This will direct the user's query to one of the following: a vector store, a web search engine, or neither (None).</p> \n</div>","metadata":{}},{"cell_type":"code","source":"class VectorStore(BaseModel):\n    (\n        \"A vectorstore contains information about symptoms, treatment\"\n        \", risk factors and other information about malaria, type 1 and\"\n        \"type 2 diabetes and migraines\"\n    )\n\n    query: str\n\n\nclass SearchEngine(BaseModel):\n    \"\"\"A search engine for searching other medical information on the web\"\"\"\n\n    query: str\n\nclass SearchEngine(BaseModel):\n    \"\"\"A search engine for searching other medical information on the web\"\"\"\n\n    query: str\n\nrouter_prompt_template = (\n    \"You are an expert in routing user queries to either a VectorStore, SearchEngine\\n\"\n    \"Use SearchEngine for all other medical queries that are not related to malaria, diabetes, or migraines.\\n\"\n    \"The VectorStore contains information on malaria, diabetes, and migraines.\\n\"\n    'Note that if a query is not medically-related, you must output \"not medically-related\", don\\'t try to use any tool.\\n\\n'\n    \"query: {query}\"\n)\n\nllm = ChatGroq(model=\"Llama3-70b-8192\", temperature=0)\nprompt = ChatPromptTemplate.from_template(router_prompt_template)\nquestion_router = prompt | llm.bind_tools(tools=[VectorStore, SearchEngine])","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:38:35.026296Z","iopub.execute_input":"2024-06-07T21:38:35.026872Z","iopub.status.idle":"2024-06-07T21:38:35.34531Z","shell.execute_reply.started":"2024-06-07T21:38:35.026842Z","shell.execute_reply":"2024-06-07T21:38:35.344303Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response = question_router.invoke(\"What are the symptoms of chest pain?\")\n\"tool_calls\" in response.additional_kwargs","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:38:35.346594Z","iopub.execute_input":"2024-06-07T21:38:35.346919Z","iopub.status.idle":"2024-06-07T21:38:36.081131Z","shell.execute_reply.started":"2024-06-07T21:38:35.34689Z","shell.execute_reply":"2024-06-07T21:38:36.080015Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:38:36.082878Z","iopub.execute_input":"2024-06-07T21:38:36.08321Z","iopub.status.idle":"2024-06-07T21:38:36.090183Z","shell.execute_reply.started":"2024-06-07T21:38:36.08318Z","shell.execute_reply":"2024-06-07T21:38:36.089058Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n  font-size: 1em;\n  color: #333;\n  background-color: #f2f2f2;\n  padding: 10px;\n  border-radius: 10px;\n  margin: 20px;\n  animation: fadeIn 1s ease-in-out;\n  width: 600px;\n\">\n    <h1>5. Retrieved Documents Relevance Grader</h1>\n    <p style=\"font-size: 1.2em;\"\n        >This is to make sure we filter the documents down so that only the relevant chunks are used as context and not the entire retrieved chunks.</p> \n</div>","metadata":{}},{"cell_type":"code","source":"from langchain_core.pydantic_v1 import validator\n\n\nclass Grader(BaseModel):\n    \"Use this format to give a binary score for relevance check on retrived documents.\"\n\n    grade: Literal[\"relevant\", \"irrelevant\"] = Field(\n        ...,\n        description=\"The relevance score for the document.\\n\"\n        \"Set this to 'relevant' if the given context is relevant to the user's query, or 'irrlevant' if the document is not relevant.\",\n    )\n\n    @validator(\"grade\", pre=True)\n    def validate_grade(cls, value):\n        if value == \"not relevant\":\n            return \"irrelevant\"\n        return value\n\n\ngrader_system_prompt_template = \"\"\"\"You are a grader tasked with assessing the relevance of a given context to a query. \n    If the context is relevant to the query, score it as \"relevant\". Otherwise, give \"irrelevant\".\n    Do not answer the actual answer, just provide the grade in JSON format with \"grade\" as the key, without any additional explanation.\"\n    \"\"\"\n\ngrader_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", grader_system_prompt_template),\n        (\"human\", \"context: {context}\\n\\nquery: {query}\"),\n    ]\n)\n\n\ngrader_chain = grader_prompt | llm.with_structured_output(Grader, method=\"json_mode\")\n\nquery = \"symptoms of migraine\"\ncontext = retriever.get_relevant_documents(query)\n\nresponse = grader_chain.invoke({\"query\": query, \"context\": context})","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:38:36.091807Z","iopub.execute_input":"2024-06-07T21:38:36.092224Z","iopub.status.idle":"2024-06-07T21:39:13.316213Z","shell.execute_reply.started":"2024-06-07T21:38:36.092185Z","shell.execute_reply":"2024-06-07T21:39:13.314935Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(response)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:39:13.31824Z","iopub.execute_input":"2024-06-07T21:39:13.318665Z","iopub.status.idle":"2024-06-07T21:39:13.325171Z","shell.execute_reply.started":"2024-06-07T21:39:13.318631Z","shell.execute_reply":"2024-06-07T21:39:13.323862Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"Treatment of Ulcer\"\ncontext = retriever.get_relevant_documents(query)\n\nresponse = grader_chain.invoke({\"query\": query, \"context\": context})\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:39:13.326562Z","iopub.execute_input":"2024-06-07T21:39:13.326943Z","iopub.status.idle":"2024-06-07T21:39:49.969505Z","shell.execute_reply.started":"2024-06-07T21:39:13.326912Z","shell.execute_reply":"2024-06-07T21:39:49.968208Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n  font-size: 1em;\n  color: #333;\n  background-color: #f2f2f2;\n  padding: 10px;\n  border-radius: 10px;\n  margin: 20px;\n  animation: fadeIn 1s ease-in-out;\n  width: 500px;\n\">\n    <h1>5. RAG Chain</h1>\n    <p style=\"font-size: 1.2em;\"\n        >Responding to the user's query based on the filtered chunks</p> \n</div>","metadata":{}},{"cell_type":"code","source":"rag_template_str = (\n    \"You are a helpful assistant. Answer the query below based only on the provided context.\\n\\n\"\n    \"context: {context}\\n\\n\"\n    \"query: {query}\"\n)\n\n\nrag_prompt = ChatPromptTemplate.from_template(rag_template_str)\nrag_chain = rag_prompt | llm | StrOutputParser()\n\nquery = \"What are the symptoms of malaria?\"\ncontext = retriever.get_relevant_documents(query)\n\nresponse = rag_chain.invoke({\"query\": query, \"context\": context})\n\nMarkdown(response)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:39:49.975339Z","iopub.execute_input":"2024-06-07T21:39:49.97571Z","iopub.status.idle":"2024-06-07T21:40:27.245803Z","shell.execute_reply.started":"2024-06-07T21:39:49.975681Z","shell.execute_reply":"2024-06-07T21:40:27.244622Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n  font-size: 1em;\n  color: #333;\n  background-color: #f2f2f2;\n  padding: 10px;\n  border-radius: 10px;\n  margin: 20px;\n  animation: fadeIn 1s ease-in-out;\n  width: 500px;\n\">\n    <h1>6. Fallback Chain</h1>\n    <p style=\"font-size: 1.2em;\"\n        >This chain is responsible for handling situations where the agent does not call a tool bacause calling a tool is not relevant.</p> \n</div>","metadata":{}},{"cell_type":"code","source":"fallback_prompt = ChatPromptTemplate.from_template(\n    (\n        \"You are a friendly medical assistant created by RedxAI.\\n\"\n        \"Do not respond to queries that are not related to health.\\n\"\n        \"If a query is not related to health, acknowledge your limitations.\\n\"\n        \"Provide concise responses to only medically-related queries.\\n\\n\"\n        \"Current conversations:\\n\\n{chat_history}\\n\\n\"\n        \"human: {query}\"\n    )\n)\n\nfallback_chain = (\n    {\n        \"chat_history\": lambda x: \"\\n\".join(\n            [\n                (\n                    f\"human: {msg.content}\"\n                    if isinstance(msg, HumanMessage)\n                    else f\"AI: {msg.content}\"\n                )\n                for msg in x[\"chat_history\"]\n            ]\n        ),\n        \"query\": itemgetter(\"query\") ,\n    }\n    | fallback_prompt\n    | llm\n    | StrOutputParser()\n)\n\nfallback_chain.invoke(\n    {\n        \"query\": \"Hello\",\n        \"chat_history\": [],\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:40:27.24736Z","iopub.execute_input":"2024-06-07T21:40:27.247792Z","iopub.status.idle":"2024-06-07T21:40:27.530686Z","shell.execute_reply.started":"2024-06-07T21:40:27.247749Z","shell.execute_reply":"2024-06-07T21:40:27.529488Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n  font-size: 1em;\n  color: #333;\n  background-color: #f2f2f2;\n  padding: 10px;\n  border-radius: 10px;\n  margin: 20px;\n  animation: fadeIn 1s ease-in-out;\n  width: 720px;\n\">\n    <h1>7. Hallucination and Answer Relevance Assessment</h1>\n    <p style=\"font-size: 1.2em;\"\n        >In the final step, we use the <strong>LLM</strong> to evaluate whether its response is a hallucination or not, based on the\n        response itself and the provided context. If the response is not derived from the given context, it is marked as a hallucination.\n        Once the hallucination check is passed, the user's query and the LLM's response are then assessed to determine if the response\n        relevant to the query.<br><br>\n        If the response is identified as a hallucination, we go back to the <strong>RAG chain</strong>. If the response is not\n        hallucination, but the answer relevance check fails, we perform a web search. Otherwise, we return the LLM's response to the user\n    </p> \n</div>","metadata":{}},{"cell_type":"code","source":"class HallucinationGrader(BaseModel):\n    \"Binary score for hallucination check in llm's response\"\n\n    grade: Literal[\"yes\", \"no\"] = Field(\n        ..., description=\"'yes' if the llm's reponse is hallucinated otherwise 'no'\"\n    )\n\n\nhallucination_grader_system_prompt_template = (\n    \"You are a grader assessing whether a response from an llm is based on a given context.\\n\"\n    \"If the llm's response is not based on the given context give a score of 'yes' meaning it's a hallucination\"\n    \"otherwise give 'no'\\n\"\n    \"Just give the grade in json with 'grade' as a key and a binary value of 'yes' or 'no' without additional explanation\"\n)\n\nhallucination_grader_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", hallucination_grader_system_prompt_template),\n        (\"human\", \"context: {context}\\n\\nllm's response: {response}\"),\n    ]\n)\n\n\nhallucination_grader_chain = (\n    RunnableParallel(\n        {\n            \"response\": itemgetter(\"response\"),\n            \"context\": lambda x: \"\\n\\n\".join([c.page_content for c in x[\"context\"]]),\n        }\n    )\n    | hallucination_grader_prompt\n    | llm.with_structured_output(HallucinationGrader, method=\"json_mode\")\n)\n\nquery = \"Symptoms of malaria\"\ncontext = retriever.get_relevant_documents(query)\nresponse = \"\"\"Based on the context provided, the symptoms of malaria include: Impaired consciousness, Convulsions, Difficulty breathing,\nSerious tiredness and fatigue, Dark or bloody urine, Yellow eyes and skin (jaundice), Abnormal bleeding, High fever, Chills,\nSweating, Nausea or vomiting, Headache, Diarrhea\"\"\"\n\nresponse = hallucination_grader_chain.invoke({\"response\": response, \"context\": context})","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:40:27.532031Z","iopub.execute_input":"2024-06-07T21:40:27.532376Z","iopub.status.idle":"2024-06-07T21:41:04.574058Z","shell.execute_reply.started":"2024-06-07T21:40:27.532347Z","shell.execute_reply":"2024-06-07T21:41:04.573002Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:41:04.575367Z","iopub.execute_input":"2024-06-07T21:41:04.575684Z","iopub.status.idle":"2024-06-07T21:41:04.583968Z","shell.execute_reply.started":"2024-06-07T21:41:04.575656Z","shell.execute_reply":"2024-06-07T21:41:04.582573Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AnswerGrader(BaseModel):\n    \"Binary score for an answer check based on a query.\"\n\n    grade: Literal[\"yes\", \"no\"] = Field(\n        ...,\n        description=\"'yes' if the provided answer is an actual answer to the query otherwise 'no'\",\n    )\n\n\nanswer_grader_system_prompt_template = (\n    \"You are a grader assessing whether a provided answer is in fact an answer to the given query.\\n\"\n    \"If the provided answer does not answer the query give a score of 'no' otherwise give 'yes'\\n\"\n    \"Just give the grade in json with 'grade' as a key and a binary value of 'yes' or 'no' without additional explanation\"\n)\n\nanswer_grader_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", answer_grader_system_prompt_template),\n        (\"human\", \"query: {query}\\n\\nanswer: {response}\"),\n    ]\n)\n\n\nanswer_grader_chain = answer_grader_prompt | llm.with_structured_output(\n    AnswerGrader, method=\"json_mode\"\n)\n\nquery = \"Symptoms of malaria\"\n# context = retriever.get_relevant_documents(query)\nresponse = \"\"\"Based on the context provided, the symptoms of malaria include: Impaired consciousness, Convulsions, Difficulty breathing,\nSerious tiredness and fatigue, Dark or bloody urine, Yellow eyes and skin (jaundice), Abnormal bleeding, High fever, Chills,\nSweating, Nausea or vomiting, Headache, Diarrhea\"\"\"\n\nresponse = answer_grader_chain.invoke({\"response\": response, \"query\": query})","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:41:04.586051Z","iopub.execute_input":"2024-06-07T21:41:04.586424Z","iopub.status.idle":"2024-06-07T21:41:04.809207Z","shell.execute_reply.started":"2024-06-07T21:41:04.586393Z","shell.execute_reply":"2024-06-07T21:41:04.807358Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:41:04.810883Z","iopub.execute_input":"2024-06-07T21:41:04.811343Z","iopub.status.idle":"2024-06-07T21:41:04.820697Z","shell.execute_reply.started":"2024-06-07T21:41:04.811301Z","shell.execute_reply":"2024-06-07T21:41:04.819432Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n  font-size: 1em;\n  color: #333;\n  background-color: #f2f2f2;\n  padding: 10px;\n  border-radius: 10px;\n  margin: 20px;\n  animation: fadeIn 1s ease-in-out;\n  width: 500px;\n\">\n    <h1>8. Defining the Agent's Workflow</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"from typing import TypedDict, Annotated\nfrom langchain_core.documents import Document\nfrom langchain_community.tools import DuckDuckGoSearchResults\nfrom langgraph.prebuilt import ToolInvocation, ToolExecutor\nfrom langchain_core.tools import Tool\nfrom langchain_core.messages.base import BaseMessage\nimport operator\n\n# ddg_search = DuckDuckGoSearchResults()\ntavily_search = TavilySearchResults()\ntool_executor = ToolExecutor(\n    tools=[\n        Tool(\n            name=\"VectorStore\",\n            func=retriever.invoke,\n            description=\"Useful to search the vector database\",\n        ),\n        Tool(\n            name=\"SearchEngine\", func=tavily_search, description=\"Useful to search the web\"\n        ),\n    ]\n)\n\n\nclass AgentSate(TypedDict):\n    \"\"\"The dictionary keeps track of the data required by the various nodes in the graph\"\"\"\n\n    query: str\n    chat_history:list[BaseMessage]\n    generation: str\n    documents: list[Document]\n\n\ndef retrieve_node(state: dict) -> dict[str, list[Document] | str]:\n    \"\"\"\n    Retrieve relevent documents from the vectorstore\n\n    query: str\n\n    return list[Document]\n    \"\"\"\n    query = state[\"query\"]\n    documents = retriever.invoke(input=query)\n    return {\"documents\": documents}\n\n\ndef fallback_node(state: dict):\n    \"\"\"\n    Fallback to this node when there is no tool call\n    \"\"\"\n    query = state[\"query\"]\n    chat_history = state[\"chat_history\"]\n    generation = fallback_chain.invoke({\"query\": query, \"chat_history\": chat_history})\n    return {\"generation\": generation}\n\n\ndef filter_documents_node(state: dict):\n    filtered_docs = list()\n\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    for i, doc in enumerate(documents, start=1):\n        grade = grader_chain.invoke({\"query\": query, \"context\": doc})\n        if grade.grade == \"relevant\":\n            print(f\"---CHUCK {i}: RELEVANT---\")\n            filtered_docs.append(doc)\n        else:\n            print(f\"---CHUCK {i}: NOT RELEVANT---\")\n    return {\"documents\": filtered_docs}\n\n\ndef rag_node(state: dict):\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n\n    generation = rag_chain.invoke({\"query\": query, \"context\": documents})\n    return {\"generation\": generation}\n\n\ndef web_search_node(state: dict):\n    query = state[\"query\"]\n    results = tavily_search.invoke(query)\n    # results = parse_search_research(results)\n    documents = [\n        Document(page_content=doc[\"content\"], metadata={\"source\": doc[\"url\"]})\n        for doc in results\n    ]\n    return {\"documents\": documents}\n\n\ndef question_router_node(state: dict):\n    query = state[\"query\"]\n    try:\n        response = question_router.invoke({\"query\": query})\n    except Exception:\n        return \"llm_fallback\"\n\n    if \"tool_calls\" not in response.additional_kwargs:\n        print(\"---No tool called---\")\n        return \"llm_fallback\"\n\n    if len(response.additional_kwargs[\"tool_calls\"]) == 0:\n        raise \"Router could not decide route!\"\n\n    route = response.additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"]\n    if route == \"VectorStore\":\n        print(\"---Routing to VectorStore---\")\n        return \"VectorStore\"\n    elif route == \"SearchEngine\":\n        print(\"---Routing to SearchEngine---\")\n        return \"SearchEngine\"\n\n\ndef should_generate(state: dict):\n    filtered_docs = state[\"documents\"]\n\n    if not filtered_docs:\n        print(\"---All retrived documents not relevant---\")\n        return \"SearchEngine\"\n    else:\n        print(\"---Some retrived documents are relevant---\")\n        return \"generate\"\n\n\ndef hallucination_and_answer_relevance_check(state: dict):\n    llm_response = state[\"generation\"]\n    documents = state[\"documents\"]\n    query = state[\"query\"]\n\n    hallucination_grade = hallucination_grader_chain.invoke(\n        {\"response\": llm_response, \"context\": documents}\n    )\n    if hallucination_grade.grade == \"no\":\n        print(\"---Hallucination check passed---\")\n        answer_relevance_grade = answer_grader_chain.invoke(\n            {\"response\": llm_response, \"query\": query}\n        )\n        if answer_relevance_grade.grade == \"yes\":\n            print(\"---Answer is relevant to question---\\n\")\n            return \"useful\"\n        else:\n            print(\"---Answer is not relevant to question---\")\n            return \"not useful\"\n    print(\"---Hallucination check failed---\")\n    return \"generate\"","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:41:04.822158Z","iopub.execute_input":"2024-06-07T21:41:04.822539Z","iopub.status.idle":"2024-06-07T21:41:04.918503Z","shell.execute_reply.started":"2024-06-07T21:41:04.822507Z","shell.execute_reply":"2024-06-07T21:41:04.917455Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langgraph.graph import StateGraph, END\n\nworkflow = StateGraph(AgentSate)\nworkflow.add_node(\"VectorStore\", retrieve_node)\nworkflow.add_node(\"SearchEngine\", web_search_node)\nworkflow.add_node(\"filter_docs\", filter_documents_node)\nworkflow.add_node(\"fallback\", fallback_node)\nworkflow.add_node(\"rag\", rag_node)\n\nworkflow.set_conditional_entry_point(\n    question_router_node,\n    {\n        \"llm_fallback\": \"fallback\",\n        \"VectorStore\": \"VectorStore\",\n        \"SearchEngine\": \"SearchEngine\",\n    },\n)\n\nworkflow.add_edge(\"VectorStore\", \"filter_docs\")\nworkflow.add_edge(\"SearchEngine\", \"filter_docs\")\nworkflow.add_conditional_edges(\n    \"filter_docs\", should_generate, {\"SearchEngine\": \"SearchEngine\", \"generate\": \"rag\"}\n)\nworkflow.add_conditional_edges(\n    \"rag\",\n    hallucination_and_answer_relevance_check,\n    {\"useful\": END, \"not useful\": \"SearchEngine\", \"generate\": \"rag\"},\n)\n\nworkflow.add_edge(\"fallback\", END)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:41:04.919815Z","iopub.execute_input":"2024-06-07T21:41:04.920117Z","iopub.status.idle":"2024-06-07T21:41:04.930502Z","shell.execute_reply.started":"2024-06-07T21:41:04.920091Z","shell.execute_reply":"2024-06-07T21:41:04.929192Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"app = workflow.compile(debug=False)\nplot = app.get_graph().draw_mermaid_png()\n\nwith open(\"plot.png\", \"wb\") as fp:\n    fp.write(plot)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:41:04.931883Z","iopub.execute_input":"2024-06-07T21:41:04.932309Z","iopub.status.idle":"2024-06-07T21:41:05.965986Z","shell.execute_reply.started":"2024-06-07T21:41:04.932251Z","shell.execute_reply":"2024-06-07T21:41:05.964994Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from io import BytesIO\nfrom PIL import Image\nfrom IPython.display import display\n\n# Assuming you have the byte image stream in the 'byte_image' variable\nimg = Image.open(BytesIO(plot))\ndisplay(img)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:41:05.967374Z","iopub.execute_input":"2024-06-07T21:41:05.967772Z","iopub.status.idle":"2024-06-07T21:41:06.023459Z","shell.execute_reply.started":"2024-06-07T21:41:05.967735Z","shell.execute_reply":"2024-06-07T21:41:06.022353Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n  font-size: 1em;\n  color: #333;\n  background-color: #f2f2f2;\n  padding: 10px;\n  border-radius: 10px;\n  margin: 20px;\n  animation: fadeIn 1s ease-in-out;\n  width: 200px;\n\">\n    <h1>9. Testing</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"for token in app.stream({\"query\": \"What are the risk factors of migraines\", \"chat_history\": []}):\n    print(token, end=\"\", flush=True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-07T21:41:06.024973Z","iopub.execute_input":"2024-06-07T21:41:06.025302Z","iopub.status.idle":"2024-06-07T21:41:47.622968Z","shell.execute_reply.started":"2024-06-07T21:41:06.025253Z","shell.execute_reply":"2024-06-07T21:41:47.621786Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response = app.invoke({\"query\": \"What are the symptoms of hypertension\", \"chat_history\": []})\nMarkdown(response[\"generation\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:41:47.624413Z","iopub.execute_input":"2024-06-07T21:41:47.624752Z","iopub.status.idle":"2024-06-07T21:41:52.101162Z","shell.execute_reply.started":"2024-06-07T21:41:47.624721Z","shell.execute_reply":"2024-06-07T21:41:52.099811Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nfrom uuid import uuid4\n\nhistory = {}\nsession_id = str(uuid4())\n\ndef chat(query):\n\n    # Initialize the chat history for the current session\n    if session_id not in history:\n        history[session_id] = []\n\n    chat_history = history[session_id]\n\n    # Invoke the app with the current query and chat history\n    result = app.invoke({\"query\": query, \"chat_history\": chat_history})\n\n    # Separate the response from the retrieved documents\n    response = result[\"generation\"]\n    documents = result[\"documents\"]\n\n    # Add the current exchange to the chat history\n    chat_history.extend([HumanMessage(content=query), AIMessage(content=response)])\n\n    if not documents:\n        return response, documents\n\n    documents = [\n        f\"{doc.page_content}\\nsource: {doc.metadata['source']}\" for doc in documents\n    ]\n\n    return response, \"\\n\\n\".join(documents)\n\n# Create the Gradio interface\ndemo = gr.Interface(\n    fn=chat,\n    inputs=gr.Textbox(label=\"Question\"),\n    outputs=[gr.Textbox(label=\"Response\"), gr.Textbox(label=\"Retrieved Documents\")],\n    title=\"RAG Chatbot\",\n    description=\"Ask a health-related query and the chatbot will generate a response using Retrieval Augmented Generation.\",\n)\n\nif __name__ == \"__main__\":\n    demo.launch(share=True, inline=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:41:52.102483Z","iopub.execute_input":"2024-06-07T21:41:52.102817Z","iopub.status.idle":"2024-06-07T21:41:58.597819Z","shell.execute_reply.started":"2024-06-07T21:41:52.102788Z","shell.execute_reply":"2024-06-07T21:41:58.596576Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reference:\n- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag_cohere.ipynb","metadata":{}},{"cell_type":"markdown","source":"<h3 style=\"text-align: center; color: #333; background-color: #f0f0f0; border-radius: 8px; border: 1px solid #ccc; padding: 10px; width: 150px;\">Thank you</h3>","metadata":{}}]}